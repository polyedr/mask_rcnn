{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1976,
     "status": "ok",
     "timestamp": 1562920210275,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "89fubZHZIcC0",
    "outputId": "4c0bbed3-0e86-44f1-cbdd-a9ebca3b2af1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/user/Desktop/Programming/Keras/Mask_R_CNN/Mask_RCNN/samples/form_login\r\n"
     ]
    }
   ],
   "source": [
    "!cd /home/user/Desktop/Programming/Keras/Mask_R_CNN/Mask_RCNN\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J5Kh4TWGILN2"
   },
   "outputs": [],
   "source": [
    "import socket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 127
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24218,
     "status": "ok",
     "timestamp": 1563089985572,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "hT6ubTLUK74x",
    "outputId": "0d198d2c-48a8-4347-9aa8-b8098d2acbae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/user/Desktop/Programming/Keras/Mask_R_CNN/mask_rcnn_venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 3296, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-4996ee3d8d09>\", line 1, in <module>\n",
      "    from google.colab import drive\n",
      "ModuleNotFoundError: No module named 'google.colab'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/Desktop/Programming/Keras/Mask_R_CNN/mask_rcnn_venv/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2033, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'ModuleNotFoundError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/Desktop/Programming/Keras/Mask_R_CNN/mask_rcnn_venv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1095, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/user/Desktop/Programming/Keras/Mask_R_CNN/mask_rcnn_venv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 313, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/user/Desktop/Programming/Keras/Mask_R_CNN/mask_rcnn_venv/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 347, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/user/.pyenv/versions/3.6.6/lib/python3.6/inspect.py\", line 1483, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/user/.pyenv/versions/3.6.6/lib/python3.6/inspect.py\", line 1441, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/user/.pyenv/versions/3.6.6/lib/python3.6/inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"/home/user/.pyenv/versions/3.6.6/lib/python3.6/inspect.py\", line 733, in getmodule\n",
      "    if ismodule(module) and hasattr(module, '__file__'):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8afJOumL0vT"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/gdrive/My Drive/mask_rcnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5196,
     "status": "ok",
     "timestamp": 1563089994475,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "d68e7N-XH4Vs",
    "outputId": "b8529f96-fc4f-4c6d-ab3b-127c3a39f8ca"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import skimage.draw\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"/content/gdrive/My Drive/mask_rcnn\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_coco.h5\")\n",
    "# Download COCO trained weights from Releases if needed\n",
    "if not os.path.exists(COCO_MODEL_PATH):\n",
    "    utils.download_trained_weights(COCO_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqbCLNnXIZLA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qf7JtvLDH4Vs"
   },
   "source": [
    "# Mask R-CNN - Train on Shapes Dataset\n",
    "\n",
    "\n",
    "This notebook shows how to train Mask R-CNN on your own dataset. To keep things simple we use a synthetic dataset of shapes (squares, triangles, and circles) which enables fast training. You'd still need a GPU, though, because the network backbone is a Resnet101, which would be too slow to train on a CPU. On a GPU, you can start to get okay-ish results in a few minutes, and good results in less than an hour.\n",
    "\n",
    "The code of the *Shapes* dataset is included below. It generates images on the fly, so it doesn't require downloading any data. And it can generate images of any size, so we pick a small image size to train faster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E9_Lj7aoH4Vx"
   },
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 953
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1563089999002,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "mq44SOp0H4Vy",
    "outputId": "72f14872-6b83-4787-ba8a-43da51810ac8"
   },
   "outputs": [],
   "source": [
    "class LoginConfig(Config):\n",
    "    \"\"\"Configuration for training on the toy shapes dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the toy shapes dataset.\n",
    "    \"\"\"\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 32\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "        # Give the configuration a recognizable name\n",
    "    NAME = \"form_login\"\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 7  # Background + form_login\n",
    "\n",
    "    # Number of training steps per epoch\n",
    "    STEPS_PER_EPOCH = 100\n",
    "\n",
    "    # Skip detections with < 90% confidence\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9\n",
    "    \n",
    "config = LoginConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXJ-19jAH4V0"
   },
   "source": [
    "## Notebook Preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zaBV_WGzH4V2"
   },
   "outputs": [],
   "source": [
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5CG5D4hBH4V3"
   },
   "source": [
    "## Dataset\n",
    "\n",
    "Create a synthetic dataset\n",
    "\n",
    "Extend the Dataset class and add a method to load the shapes dataset, `load_shapes()`, and override the following methods:\n",
    "\n",
    "* load_image()\n",
    "* load_mask()\n",
    "* image_reference()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xru-m42TH4V3"
   },
   "outputs": [],
   "source": [
    "class Form_loginDataset(utils.Dataset):\n",
    "\n",
    "    def load_form_login(self, dataset_dir, subset):\n",
    "        \"\"\"Load a subset of the Form_login dataset.\n",
    "        dataset_dir: Root directory of the dataset.\n",
    "        subset: Subset to load: train or val\n",
    "        \"\"\"\n",
    "        # Add classes. We have five classes to add.\n",
    "        self.add_class(\"form_login\", 1, \"input\")\n",
    "        self.add_class(\"form_login\", 2, \"text\")\n",
    "        self.add_class(\"form_login\", 3, \"button\")\n",
    "        self.add_class(\"form_login\", 4, \"checkbox\")\n",
    "        self.add_class(\"form_login\", 5, \"radiobutton\")\n",
    "        self.add_class(\"form_login\", 6, \"image\")\n",
    "        self.add_class(\"form_login\", 7, \"select\")\n",
    "\n",
    "        # Train or validation dataset?\n",
    "        assert subset in [\"train\", \"val\"]\n",
    "        dataset_dir = os.path.join(dataset_dir, subset)\n",
    "\n",
    "        # Load annotations\n",
    "        # VGG Image Annotator (up to version 1.6) saves each image in the form:\n",
    "        # { 'filename': '28503151_5b5b7ec140_b.jpg',\n",
    "        #   'regions': {\n",
    "        #       '0': {\n",
    "        #           'region_attributes': {},\n",
    "        #           'shape_attributes': {\n",
    "        #               'all_points_x': [...],\n",
    "        #               'all_points_y': [...],\n",
    "        #               'name': 'rectangle'}},\n",
    "        #       ... more regions ...\n",
    "        #   },\n",
    "        #   'size': 100202\n",
    "        # }\n",
    "        # We mostly care about the x and y coordinates of each region\n",
    "        # Note: In VIA 2.0, regions was changed from a dict to a list.\n",
    "        annotations = json.load(open(os.path.join(dataset_dir, \"via_export_json.json\")))\n",
    "        annotations = list(annotations.values())  # don't need the dict keys\n",
    "\n",
    "        # The VIA tool saves images in the JSON even if they don't have any\n",
    "        # annotations. Skip unannotated images.\n",
    "        annotations = [a for a in annotations if a['regions']]\n",
    "\n",
    "        # Add images\n",
    "        for a in annotations:\n",
    "            # Get the x, y coordinaets of points of the rectangles that make up\n",
    "            # the outline of each object instance. These are stores in the\n",
    "            # shape_attributes (see json format above)\n",
    "            # The if condition is needed to support VIA versions 1.x and 2.x.\n",
    "            \n",
    "            # The type of the object is list\n",
    "            '''\n",
    "            if type(a['regions']) is dict:\n",
    "                rectangles = [r['shape_attributes'] for r in a['regions'].values()]\n",
    "            else:\n",
    "                rectangles = [r['shape_attributes'] for r in a['regions']] \n",
    "            '''\n",
    "            rectangles = [r['shape_attributes'] for r in a['regions']] \n",
    "            for rectangle in rectangles:\n",
    "                print(rectangle)\n",
    "            # load_mask() needs the image size to convert rectangles to masks.\n",
    "            # Unfortunately, VIA doesn't include it in JSON, so we must read\n",
    "            # the image. This is only managable since the dataset is tiny.\n",
    "\n",
    "            objects = [r['region_attributes'] for r in a['regions']]\n",
    "\n",
    "            '''\n",
    "            # Check the presense of the HTML elements types\n",
    "            classids = [(n['HTML element']) for n in objects]\n",
    "            for classid in classids:\n",
    "                print(classid.keys())\n",
    "\n",
    "            class_ids = [int(n['HTML element']) for n in objects]\n",
    "            '''\n",
    "\n",
    "            image_path = os.path.join(dataset_dir, a['filename'])\n",
    "            image = skimage.io.imread(image_path)\n",
    "            height, width = image.shape[:2]\n",
    "\n",
    "            self.add_image(\n",
    "                \"form_login\",\n",
    "                image_id=a['filename'],  # use file name as a unique image id\n",
    "                path=image_path,\n",
    "                width=width, height=height,\n",
    "                rectangles=rectangles,\n",
    "                objects=objects)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for an image.\n",
    "       Returns:\n",
    "        masks: A bool array of shape [height, width, instance count] with\n",
    "            one mask per instance.\n",
    "        class_ids: a 1D array of class IDs of the instance masks.\n",
    "        \"\"\"\n",
    "        # If not a form_login dataset image, delegate to parent class.\n",
    "        image_info = self.image_info[image_id]\n",
    "        if image_info[\"source\"] != \"form_login\":\n",
    "            return super(self.__class__, self).load_mask(image_id)\n",
    "\n",
    "        class_names = image_info['objects']\n",
    "\n",
    "        # Convert rectangles to a bitmap mask of shape\n",
    "        # [height, width, instance_count]\n",
    "        info = self.image_info[image_id]\n",
    "        mask = np.zeros([info[\"height\"], info[\"width\"], len(info[\"rectangles\"])],\n",
    "                        dtype=np.uint8)\n",
    "        for i, p in enumerate(info[\"rectangles\"]):\n",
    "            # print(i)\n",
    "            # print(p)\n",
    "            # Get indexes of pixels inside the rectangle and set them to 1\n",
    "            # rr, cc = skimage.draw.rectangle(p['all_points_y'], p['all_points_x'])\n",
    "            # https://scikit-image.org/docs/dev/api/skimage.draw.html#skimage.draw.rectangle\n",
    "            #img = np.zeros((int(p['width']*2), int(p['height']*2)), dtype=np.uint8)\n",
    "            \n",
    "            #start = (p['x'], p['y'] - p['height'])\n",
    "            #extent = (p['width'], p['height'])\n",
    "            \n",
    "            start = (p['y'], p['x'])\n",
    "            extent = (p['height'], p['width'])\n",
    "            \n",
    "            \n",
    "            # rr, cc = skimage.draw.rectangle(start, extent=extent, shape=img.shape)\n",
    "            rr, cc = skimage.draw.rectangle(start, extent=extent)            \n",
    "            # print(rr)\n",
    "            # print(cc)\n",
    "            mask[rr, cc, i] = 1\n",
    "        # Assign class_ids by reading class_names\n",
    "        class_ids = np.zeros([len(info[\"rectangles\"])])\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID only, we return an array of 1s\n",
    "        # return mask.astype(np.bool), np.ones([mask.shape[-1]], dtype=np.int32)\n",
    "\n",
    "        # In the form_login dataset, pictures are labeled with name 'input' and 'button' representing input and button.\n",
    "        for i, p in enumerate(class_names):\n",
    "            # print('HTML element=', p['HTML element'])\n",
    "            # print(\"i=\",i)\n",
    "            # temp = p['HTML element']\n",
    "            # for key in temp:\n",
    "            #     print('temp=', temp)\n",
    "            #     print(key)\n",
    "            # \"name\" is the attributes name decided when labeling, etc. 'region_attributes': {name:'a'} // \"region_attributes\":{\"HTML element\":{\"input\":true}}}\n",
    "            '''\n",
    "            if p['HTML element'] == 'input':\n",
    "                class_ids[i] = 1\n",
    "\n",
    "            elif p['HTML element'] == 'button':\n",
    "                class_ids[i] = 2\n",
    "\n",
    "            elif p['HTML element'] == 'hyperlink':\n",
    "                class_ids[i] = 3\n",
    "\n",
    "            elif p['HTML element'] == 'text':\n",
    "                class_ids[i] = 4\n",
    "\n",
    "            elif p['HTML element'] == 'select':\n",
    "                class_ids[i] = 5\n",
    "            '''\n",
    "\n",
    "            if 'input' in p['type']:\n",
    "                class_ids[i] = 1\n",
    "\n",
    "            elif 'text' in p['type']:\n",
    "                class_ids[i] = 2\n",
    "\n",
    "            elif 'button' in p['type']:\n",
    "                class_ids[i] = 3\n",
    "\n",
    "            elif 'checkbox' in p['type']:\n",
    "                class_ids[i] = 4\n",
    "\n",
    "            elif 'radiobutton' in p['type']:\n",
    "                class_ids[i] = 5\n",
    "\n",
    "            elif 'image' in p['type']:\n",
    "                class_ids[i] = 6\n",
    "\n",
    "            elif 'select' in p['type']:\n",
    "                class_ids[i] = 7\n",
    "\n",
    "\n",
    "        # assert code here to extend to other labels\n",
    "\n",
    "        # Should return multiple classes ids\n",
    "        class_ids = np.array(class_ids, dtype=np.int32)\n",
    "\n",
    "        # class_ids = class_ids.astype(int)\n",
    "\n",
    "        return mask.astype(np.bool), class_ids\n",
    "\n",
    "    \n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the path of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"form_login\":\n",
    "            return info[\"path\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 85689,
     "status": "ok",
     "timestamp": 1563090106383,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "dzWFQ7i1H4V6",
    "outputId": "d0775cca-4fc6-4dcd-e524-fda5aade69ac"
   },
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = Form_loginDataset()\n",
    "dataset_train.load_form_login('gdrive/My Drive/Colab Notebooks/dataset','train')\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = Form_loginDataset()\n",
    "dataset_val.load_form_login('gdrive/My Drive/Colab Notebooks/dataset','train')\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2734,
     "status": "ok",
     "timestamp": 1562922613697,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "iMCFWCwbXpUe",
    "outputId": "3116b885-0d54-4513-bb1f-3a18a126809b"
   },
   "outputs": [],
   "source": [
    "!cat gdrive/My\\ Drive/Colab\\ Notebooks/dataset/train/via_export_json.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 58793,
     "status": "ok",
     "timestamp": 1562295313950,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "-n3adrtZRhnG",
    "outputId": "06f8d3a7-d6d1-45af-f368-7bbfadc498bb"
   },
   "outputs": [],
   "source": [
    "!ls gdrive/My\\ Drive/Colab\\ Notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 615
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2030,
     "status": "ok",
     "timestamp": 1563090150240,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "ElLLbP4AH4V8",
    "outputId": "b2a8b76c-a5c3-4917-99ce-358336995d96"
   },
   "outputs": [],
   "source": [
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ORfTQYsmH4V-"
   },
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 451
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9913,
     "status": "ok",
     "timestamp": 1563090165443,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "KpLYtmZJH4V_",
    "outputId": "77abe4da-5814-4983-b311-438ba8e54651"
   },
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jprr7B2AH4WB",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Which weights to start with?\n",
    "init_with = \"coco\"  # imagenet, coco, or last\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nXRly2TZH4WE"
   },
   "source": [
    "## Training\n",
    "\n",
    "Train in two stages:\n",
    "1. Only the heads. Here we're freezing all the backbone layers and training only the randomly initialized layers (i.e. the ones that we didn't use pre-trained weights from MS COCO). To train only the head layers, pass `layers='heads'` to the `train()` function.\n",
    "\n",
    "2. Fine-tune all layers. For this simple example it's not necessary, but we're including it to show the process. Simply pass `layers=\"all` to train all layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15460305,
     "status": "ok",
     "timestamp": 1563105644326,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "roZc4sz2H4WE",
    "outputId": "c739e73f-6483-4e04-aedc-95d8b1497107",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=4, \n",
    "            layers='heads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15308,
     "status": "ok",
     "timestamp": 1562299155253,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "0-xsIGrGlWCH",
    "outputId": "177c9d4a-9c74-4a1d-d79f-b3dc773f99ec"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYCY3k1cjit9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XObwlUULlWOo"
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EjLwaQoBH4WG",
    "outputId": "bbfa29ab-1dca-4da7-f451-faf7a6f577ad",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=7, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJIHbiDT8RhL"
   },
   "outputs": [],
   "source": [
    "model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    "model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4745126,
     "status": "ok",
     "timestamp": 1562317108682,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "7WvPHm_hD7WB",
    "outputId": "657105ec-080a-4d19-f213-0d948b57fe99"
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.load_weights(model_path, by_name=True)\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=3, \n",
    "            layers=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 22028,
     "status": "error",
     "timestamp": 1562317751832,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "v8ONvAw4H4WJ",
    "outputId": "4d0ec6a4-271e-47b3-8779-8bfe5e71f97d"
   },
   "outputs": [],
   "source": [
    "# Save weights\n",
    "# Typically not needed because callbacks save after every epoch\n",
    "# Uncomment to save manually\n",
    " model_path = os.path.join(MODEL_DIR, \"mask_rcnn_shapes.h5\")\n",
    " model.keras_model.save_weights(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oH8oYJJGH4WL"
   },
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 480
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5986,
     "status": "error",
     "timestamp": 1562317765964,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "uvmhyIUoH4WM",
    "outputId": "b60e2eb9-947a-4788-b80c-a764648f8050"
   },
   "outputs": [],
   "source": [
    "class InferenceConfig(LoginConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1205,
     "status": "ok",
     "timestamp": 1562310526504,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "S8jlsaV2H4WO",
    "outputId": "da438457-446f-45a0-b4d3-996780657259"
   },
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 573
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2734,
     "status": "ok",
     "timestamp": 1562310532483,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "1Iv0IUe3H4WS",
    "outputId": "27b7083f-6bee-4a99-8d3d-fca1a4db5eb6"
   },
   "outputs": [],
   "source": [
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8d8nQgkCH4WW"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 27321,
     "status": "ok",
     "timestamp": 1562310720462,
     "user": {
      "displayName": "Ilia Tereshin",
      "photoUrl": "",
      "userId": "10312814348405355083"
     },
     "user_tz": -420
    },
    "id": "kktAciiHH4WX",
    "outputId": "ee12fc05-79e9-4d35-ef9f-4fdbaa301f9c"
   },
   "outputs": [],
   "source": [
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, 10)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_id, gt_mask,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'])\n",
    "    APs.append(AP)\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IYDCWbZwH4Wa"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "train_shapes.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
